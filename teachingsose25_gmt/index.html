<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Generative Models on Text - Marion Di Marco   (née Weller)</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="Generative Models on Text" />
<meta property="og:description" content="Sommer Semester 2025
Masters Seminar: Generative Models on Text
Large Language Models (such as GPT2, GPT3, GPT4, Llama, T5) and Intelligent Chatbots (such as ChatGPT, Claude, Gemini and Copilot) are a very timely topic.
Contents: N-gram language models, neural language modeling, word2vec, RNNs, Transformers, BERT, RLHF, ChatGPT, multilingual alignment, prompting, transfer learning, domain adaptation, linguistic knowledge in large language models
Instructors: Prof. Alexander Fraser, Marion Di Marco
Location: Room D.2.11" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mariondimarco.github.io/teachingsose25_gmt/" /><meta property="article:section" content="" />
<meta property="article:published_time" content="2019-04-19T21:37:58+05:30" />
<meta property="article:modified_time" content="2019-04-19T21:37:58+05:30" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Generative Models on Text"/>
<meta name="twitter:description" content="Sommer Semester 2025
Masters Seminar: Generative Models on Text
Large Language Models (such as GPT2, GPT3, GPT4, Llama, T5) and Intelligent Chatbots (such as ChatGPT, Claude, Gemini and Copilot) are a very timely topic.
Contents: N-gram language models, neural language modeling, word2vec, RNNs, Transformers, BERT, RLHF, ChatGPT, multilingual alignment, prompting, transfer learning, domain adaptation, linguistic knowledge in large language models
Instructors: Prof. Alexander Fraser, Marion Di Marco
Location: Room D.2.11"/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="https://mariondimarco.github.io/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://mariondimarco.github.io/css/main.css" /><link rel="stylesheet" type="text/css" href="https://mariondimarco.github.io/css/dark.css" media="(prefers-color-scheme: dark)" />

	
	<script src="https://mariondimarco.github.io/js/main.js"></script>
</head>

<body>
	<div class="container wrapper post">
		<div class="header">
	<h1 class="site-title"><a href="/">Marion Di Marco   (née Weller)</a></h1>
	<div class="site-description"><h2>Post-Doc Researcher at TUM (School of Computation, Information and Technology)</h2><nav class="nav social">
			<ul class="flat"></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/home">Home</a>
			</li>
			
			<li>
				<a href="/publications">Publications</a>
			</li>
			
			<li>
				<a href="/teaching">Teaching</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post-header">
			<h1 class="title">Generative Models on Text</h1>
		</div>

		<div class="markdown">
			<p><strong>Sommer Semester 2025</strong></p>
<p><strong>Masters Seminar: Generative Models on Text</strong></p>
<p>Large Language Models (such as GPT2, GPT3, GPT4, Llama, T5) and Intelligent Chatbots (such as ChatGPT, Claude, Gemini and Copilot) are a very timely topic.</p>
<p>Contents:
N-gram language models, neural language modeling, word2vec, RNNs, Transformers, BERT, RLHF, ChatGPT, multilingual alignment, prompting, transfer learning, domain adaptation, linguistic knowledge in large language models</p>
<p><strong>Instructors</strong>: <a href="https://alexfraser.github.io/">Prof. Alexander Fraser</a>, <a href="https://mariondimarco.github.io">Marion Di Marco</a></p>
<hr>
<p><strong>Location:</strong>
Room D.2.11</p>
<p><strong>Time:</strong>
Tuesday 16:15 &ndash; 17:45</p>
<hr>
<p><strong>Lectures</strong></p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center"> </th>
<th style="text-align:left"></th>
<th> </th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Lecture 1</td>
<td style="text-align:center"></td>
<td style="text-align:left">29. 04. 2025</td>
<td></td>
<td><a href="https://mariondimarco.github.io/Lectures/Organization_SoSe25.pdf">Organization</a> and <a href="https://mariondimarco.github.io/Lectures/lecture_Introduction_SoSe25.pdf">Introduction to Linguistic Concepts</a></td>
</tr>
<tr>
<td style="text-align:center"> </td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">Lecture 2</td>
<td style="text-align:center"></td>
<td style="text-align:left">06. 05. 2025</td>
<td></td>
<td><a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">N-gram Models</a> (without section 3.7)</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td>(Dan Jurafsky and James H. Martin (2025). Speech and Language Processing)</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td><a href="https://mariondimarco.github.io/Lectures/lecture_ngrams_SoSe25.pdf">Slides</a></td>
</tr>
<tr>
<td style="text-align:center"> </td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">Lecture 3</td>
<td style="text-align:center"></td>
<td style="text-align:left">13. 05. 2025</td>
<td></td>
<td><a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">Bengio et al. (2003): A Neural Probabilistic Language Model.</a></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td>(Journal of Machine Learning Research 3, 1137-1155)</td>
</tr>
<tr>
<td style="text-align:center"> </td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">Lecture 4</td>
<td style="text-align:center"></td>
<td style="text-align:left">20. 05.2025</td>
<td></td>
<td>Talk by Prof. Victoria Nash: <a href="https://chn.tum.de/event/event/ai-and-evolution-of-digital-childhood.">AI and the Evolution of Digital Childhood</a></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td>May 20, 2025  16:15-18:00  in Room D.0.0.1</td>
</tr>
<tr>
<td style="text-align:center"> </td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">Lecture 5</td>
<td style="text-align:center"></td>
<td style="text-align:left">27. 05. 2025</td>
<td></td>
<td><a href="https://arxiv.org/abs/1902.06006">Smith (2019): Contextual Word Representations: A Contextual Introduction.</a></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td>(arXiv)</td>
</tr>
<tr>
<td style="text-align:center"> </td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">Lecture 6</td>
<td style="text-align:center"></td>
<td style="text-align:left">03. 06. 2025</td>
<td></td>
<td>Lena Voita. NLP Course: <a href="https://lena-voita.github.io/nlp_course/language_modeling.html">Neural Language Models</a> and</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td><a href="https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html">Sequence to Sequence and Attention</a>  (Web Tutorial)</td>
</tr>
<tr>
<td style="text-align:center"> </td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:left">10. 06. 2025</td>
<td></td>
<td>Whitsun Vacation &ndash; no lecture</td>
</tr>
<tr>
<td style="text-align:center"> </td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">Lecture 7</td>
<td style="text-align:center"></td>
<td style="text-align:left">17. 06. 2025</td>
<td></td>
<td><a href="https://arxiv.org/abs/1706.03762">Vaswani et al. (2017): Attention Is All You Need</a> (NIPS)</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td>Lecture by Dr. Lukas Edman <a href="https://mariondimarco.github.io/Lectures/lecture_Attention_and_Transformers.pdf">slides</a></td>
</tr>
<tr>
<td style="text-align:center"> </td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">Lecture 8</td>
<td style="text-align:center"></td>
<td style="text-align:left">24. 06. 2025</td>
<td></td>
<td><a href="https://aclanthology.org/N19-1423/">Devlin et al. (2019): BERT:  Pre-training of Deep Bidirectional Transformers</a></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:center"> </td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">Lecture 9</td>
<td style="text-align:center"></td>
<td style="text-align:left">01. 07. 2025</td>
<td></td>
<td><a href="https://arxiv.org/abs/2203.02155">Ouyang et al. (2022): Training language models to follow instructions </a></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td><a href="https://arxiv.org/abs/2203.02155"> with human feedback.</a> (arXiv) <a href="https://mariondimarco.github.io/Lectures/lecture_rlhf.pdf">slides</a></td>
</tr>
<tr>
<td style="text-align:center"> </td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">Lecture 10</td>
<td style="text-align:center"></td>
<td style="text-align:left">08. 07. 2025</td>
<td></td>
<td>Paper Presentations</td>
</tr>
<tr>
<td style="text-align:center"> </td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">Lecture 11</td>
<td style="text-align:center"></td>
<td style="text-align:left">15. 07. 2025</td>
<td></td>
<td>Paper Presentations</td>
</tr>
<tr>
<td style="text-align:center"> </td>
<td style="text-align:center"></td>
<td style="text-align:left"></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">Lecture 12</td>
<td style="text-align:center"></td>
<td style="text-align:left">22. 07. 2025</td>
<td></td>
<td>Paper Presentations</td>
</tr>
</tbody>
</table>
<hr>
<p><strong>Literature</strong></p>
<p><a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing</a><br>
Dan Jurafsky and James H. Martin (2024; 3rd ed. draft)</p>
<hr>
<p><strong>Paper Presentations</strong></p>
<p>Please select a paper from the list below by letting me know per mail your two top choices.
Alternatively, you can select another paper that is relevant to the seminar &ndash; in this case, please also contact me by mail.</p>
<p>Contact: marion.dimarco &ndash;AT&ndash; tum.de</p>
<ul>
<li>
<p><del>Xu et al. (2025): <a href="https://aclanthology.org/2025.naacl-long.172.pdf">LLM The Genius Paradox: A Linguistic and Math Expert’s Struggle with Simple Word-based Counting Problems.</a> NAACL 2025.</del></p>
</li>
<li>
<p><del>Hou et al. (2023): <a href="https://aclanthology.org/2023.emnlp-main.459.pdf">Effects of sub-word segmentation on performance of transformer language models.</a> EMNLP 2023.</del></p>
</li>
<li>
<p>Hu et al. (2025): <a href="https://arxiv.org/abs/2502.19249">Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases.</a> ACL 2025.</p>
</li>
<li>
<p>Helm et al. (2025): <a href="https://arxiv.org/abs/2503.09202">Token Weighting for Long-Range Language Modeling</a>. NAACL 2025 (findings).</p>
</li>
<li>
<p><del>Luo et al. (2025): <a href="https://arxiv.org/abs/2502.05867">Self-Training Large Language Models for Tool-Use Without Demonstrations.</a> arXiv.</del></p>
</li>
<li>
<p><del>Kang et al. (2025): <a href="https://aclanthology.org/2025.naacl-long.183.pdf">Unfamiliar Finetuning Examples Control How Language Models Hallucinate.</a> NAACL 2025.</del></p>
</li>
<li>
<p><del>Hu et al. (2025): <a href="https://aclanthology.org/2025.naacl-long.288.pdf">Fine-Tuning Large Language Models with Sequential Instructions</a>. NAACL 2025.</del></p>
</li>
<li>
<p>Zhang et al (2025): <a href="https://aclanthology.org/2025.findings-naacl.98.pdf">Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?</a>. NAACL 2025 (findings).</p>
</li>
<li>
<p><del>Mondshine et al. (2025): <a href="https://aclanthology.org/2025.findings-naacl.73.pdf">Beyond English: The Impact of Prompt Translation Strategies across Languages and Tasks in Multilingual LLMs</a>. NAACL 2025 (findings).</del></p>
</li>
</ul>
<hr>
<p><strong>Presentation details:</strong> 20 minutes presentation + discussion time</p>
<p><strong>Written summary:</strong> 6 pages + references. Please use the <a href="https://www.overleaf.com/latex/templates/association-for-computational-linguistics-acl-conference/jvxskxpnznfj">ACL template</a></p>
<p><strong>Summary deadline:</strong> 3 weeks after the presentation</p>

		</div>

		<div class="post-tags">
			
		</div></div>
	<div class="footer wrapper">
	<nav class="nav">
		<div> © Copyright notice |  <a href="https://github.com/vividvilla/ezhil">Ezhil theme</a> | Built with <a href="https://gohugo.io">Hugo</a></div>
	</nav>
</div>


</body>
</html>
